{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup\nInstalling needed libraries","metadata":{}},{"cell_type":"code","source":"! pip install numpy==1.26.4 scikit-learn==1.2.2 fastprogress==1.0.3 transformers==4.38.1 pandas==2.1.4 tqdm==4.66.1 torch==2.1.2 torchmetrics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-06T16:31:40.787115Z","iopub.execute_input":"2024-08-06T16:31:40.787389Z","iopub.status.idle":"2024-08-06T16:32:14.087694Z","shell.execute_reply.started":"2024-08-06T16:31:40.787364Z","shell.execute_reply":"2024-08-06T16:32:14.086765Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy==1.26.4 in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: scikit-learn==1.2.2 in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: fastprogress==1.0.3 in /opt/conda/lib/python3.10/site-packages (1.0.3)\nCollecting transformers==4.38.1\n  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pandas==2.1.4\n  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting tqdm==4.66.1\n  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch==2.1.2 in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.0.post0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (3.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (0.23.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (2.32.3)\nCollecting tokenizers<0.19,>=0.14 (from transformers==4.38.1)\n  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (0.4.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas==2.1.4) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.1.4) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.1.4) (2023.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2) (2024.5.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.3.post0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.38.1) (3.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.2) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2) (1.3.0)\nDownloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tqdm, pandas, tokenizers, transformers\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.66.4\n    Uninstalling tqdm-4.66.4:\n      Successfully uninstalled tqdm-4.66.4\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.3 which is incompatible.\nconda 24.5.0 requires packaging>=23.0, but you have packaging 21.3 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndatasets 2.20.0 requires tqdm>=4.66.3, but you have tqdm 4.66.1 which is incompatible.\nfeaturetools 1.31.0 requires tqdm>=4.66.3, but you have tqdm 4.66.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nxarray 2024.6.0 requires packaging>=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pandas-2.1.4 tokenizers-0.15.2 tqdm-4.66.1 transformers-4.38.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# optional\n! pip install git+https://github.com/Haruray/ekphrasis-custom.git","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:32:14.089599Z","iopub.execute_input":"2024-08-06T16:32:14.089938Z","iopub.status.idle":"2024-08-06T16:32:32.852303Z","shell.execute_reply.started":"2024-08-06T16:32:14.089908Z","shell.execute_reply":"2024-08-06T16:32:32.851228Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/Haruray/ekphrasis-custom.git\n  Cloning https://github.com/Haruray/ekphrasis-custom.git to /tmp/pip-req-build-_koevexq\n  Running command git clone --filter=blob:none --quiet https://github.com/Haruray/ekphrasis-custom.git /tmp/pip-req-build-_koevexq\n  Resolved https://github.com/Haruray/ekphrasis-custom.git to commit 33f15ce8eef096efe40b134aac0c06f437cd9a03\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from ekphrasis==0.5.52) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from ekphrasis==0.5.52) (4.66.1)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from ekphrasis==0.5.52) (0.4.6)\nRequirement already satisfied: ujson in /opt/conda/lib/python3.10/site-packages (from ekphrasis==0.5.52) (5.10.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from ekphrasis==0.5.52) (3.7.5)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from ekphrasis==0.5.52) (3.2.4)\nCollecting ftfy (from ekphrasis==0.5.52)\n  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from ekphrasis==0.5.52) (1.26.4)\nCollecting emot (from ekphrasis==0.5.52)\n  Downloading emot-3.1-py3-none-any.whl.metadata (396 bytes)\nCollecting mtranslate (from ekphrasis==0.5.52)\n  Downloading mtranslate-1.8.tar.gz (2.4 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->ekphrasis==0.5.52) (0.2.13)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->ekphrasis==0.5.52) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->ekphrasis==0.5.52) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->ekphrasis==0.5.52) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->ekphrasis==0.5.52) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->ekphrasis==0.5.52) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->ekphrasis==0.5.52) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->ekphrasis==0.5.52) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->ekphrasis==0.5.52) (2.9.0.post0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->ekphrasis==0.5.52) (1.16.0)\nDownloading emot-3.1-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: ekphrasis, mtranslate\n  Building wheel for ekphrasis (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ekphrasis: filename=ekphrasis-0.5.52-py3-none-any.whl size=617095 sha256=8369c674a0d03933a1f871f758c509c0ca7d3eb837ebeca444a180a1b4d6f0bc\n  Stored in directory: /tmp/pip-ephem-wheel-cache-lwea23m2/wheels/db/aa/78/3324febc0249d71447c15e7cdf65517757e106402e6d6bb448\n  Building wheel for mtranslate (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for mtranslate: filename=mtranslate-1.8-py3-none-any.whl size=3672 sha256=a57553a1d8c8e3b65649053ec8ab91745552395ae527693c879d4a08fe10d475\n  Stored in directory: /root/.cache/pip/wheels/c2/04/15/d7654c2c4a9a52e09922967593f3278fed66059be65ca671ea\nSuccessfully built ekphrasis mtranslate\nInstalling collected packages: mtranslate, emot, ftfy, ekphrasis\nSuccessfully installed ekphrasis-0.5.52 emot-3.1 ftfy-6.2.3 mtranslate-1.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:32:32.853804Z","iopub.execute_input":"2024-08-06T16:32:32.854190Z","iopub.status.idle":"2024-08-06T16:32:32.858801Z","shell.execute_reply.started":"2024-08-06T16:32:32.854152Z","shell.execute_reply":"2024-08-06T16:32:32.857745Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Cloning Source Code Repository","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/\nif not os.path.isdir(\"TA-Multi-Label-Emotion-Classification-Indonesian-Text\"):\n  print(\"Cloning repo...\")\n  !git clone https://github.com/Haruray/TA-Multi-Label-Emotion-Classification-Indonesian-Text\n%cd /kaggle/working/TA-Multi-Label-Emotion-Classification-Indonesian-Text\n!git pull https://github.com/Haruray/TA-Multi-Label-Emotion-Classification-Indonesian-Text\n    \n%cd /kaggle/working/TA-Multi-Label-Emotion-Classification-Indonesian-Text/src","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:32:32.860805Z","iopub.execute_input":"2024-08-06T16:32:32.861089Z","iopub.status.idle":"2024-08-06T16:32:35.357807Z","shell.execute_reply.started":"2024-08-06T16:32:32.861047Z","shell.execute_reply":"2024-08-06T16:32:35.356812Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning repo...\nCloning into 'TA-Multi-Label-Emotion-Classification-Indonesian-Text'...\nremote: Enumerating objects: 52, done.\u001b[K\nremote: Counting objects: 100% (52/52), done.\u001b[K\nremote: Compressing objects: 100% (33/33), done.\u001b[K\nremote: Total 52 (delta 13), reused 52 (delta 13), pack-reused 0\u001b[K\nUnpacking objects: 100% (52/52), 199.51 KiB | 6.04 MiB/s, done.\n/kaggle/working/TA-Multi-Label-Emotion-Classification-Indonesian-Text\nFrom https://github.com/Haruray/TA-Multi-Label-Emotion-Classification-Indonesian-Text\n * branch            HEAD       -> FETCH_HEAD\nAlready up to date.\n/kaggle/working/TA-Multi-Label-Emotion-Classification-Indonesian-Text/src\n","output_type":"stream"}]},{"cell_type":"code","source":"from MLEC import Trainer, SpanEmo, DataClass, EmoRec, EarlyStopping, EvaluateOnTest\nfrom torch.utils.data import DataLoader\nimport torch\nimport datetime\nimport json\nimport numpy as np\nimport wandb\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:32:35.359458Z","iopub.execute_input":"2024-08-06T16:32:35.359817Z","iopub.status.idle":"2024-08-06T16:32:44.425453Z","shell.execute_reply.started":"2024-08-06T16:32:35.359780Z","shell.execute_reply":"2024-08-06T16:32:44.424702Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"seed = 42\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Currently using {}\".format(device))\n\ndef set_seed(seed):    \n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nset_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:32:44.426503Z","iopub.execute_input":"2024-08-06T16:32:44.426776Z","iopub.status.idle":"2024-08-06T16:32:44.499448Z","shell.execute_reply.started":"2024-08-06T16:32:44.426752Z","shell.execute_reply":"2024-08-06T16:32:44.498126Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Currently using cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"now = datetime.datetime.now()\nfilename = now.strftime(\"%Y-%m-%d %H-%M-%S\")\n# fw = open(\"../configs/\" + filename + \".json\", \"a\")\nmodel_path = filename + \".pt\"","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:32:44.501116Z","iopub.execute_input":"2024-08-06T16:32:44.501511Z","iopub.status.idle":"2024-08-06T16:32:44.513492Z","shell.execute_reply.started":"2024-08-06T16:32:44.501475Z","shell.execute_reply":"2024-08-06T16:32:44.512650Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Constants","metadata":{}},{"cell_type":"code","source":"CHECKPOINT_PATH = model_path\nMODEL_PATH = \"/kaggle/working/TA-Multi-Label-Emotion-Classification-Indonesian-Text/models/\"+filename+\"_checkpoint.pt\"\nTRAIN_FILEPATH = \"/kaggle/working/TA-Multi-Label-Emotion-Classification-Indonesian-Text/data/id_df_train_new.csv\"\nDEV_FILEPATH = \"/kaggle/working/TA-Multi-Label-Emotion-Classification-Indonesian-Text/data/id_df_dev_new.csv\"\nTEST_FILEPATH = \"/kaggle/working/TA-Multi-Label-Emotion-Classification-Indonesian-Text/data/id_df_test_new.csv\"\nMAX_LENGTH = 128\nDROPOUT_RATE = 0.1\nTOKENIZER_NAME = \"indolem/indobert-base-uncased\"\nENCODER_NAME = \"indolem/indobert-base-uncased\"\n\n# === SPANEMO PARAMS ===\nCLASSIFIER_LEARNING_RATE = 0.005\nENCODER_LEARNING_RATE = 2e-5\nTRAIN_BATCH_SIZE = 16\nEVAL_BATCH_SIZE = 16\nLCA_LOSSES = [0.6]\nZLPR_LOSSES = [0.4]\n\n# === INDOBERT PARAMS ===\n# CLASSIFIER_LEARNING_RATE = 5e-5\n# ENCODER_LEARNING_RATE = 5e-5\n# TRAIN_BATCH_SIZE = 32\n# EVAL_BATCH_SIZE = 32\n# LCA_LOSSES = [0.9]\n# ZLPR_LOSSES = [0.0]\n\nMAX_EPOCH = 20\nPATIENCE = 20\nMODEL_TO_TRAIN = \"spanemo\" #  \"emorec\" or \"spanemo\"\nUSE_PREFIX = MODEL_TO_TRAIN == \"spanemo\" # only True if its SpanEmo\nPROJECT_NAME = \"TA Revisi\" # wandb project name\n\nLCA_LOSSES = [0.0, 0.6]\nZLPR_LOSSES = [0.0, 0.4]\n\nos.environ[\"WANDB_API_KEY\"] = \"\" # enter wandb api key here","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:32:44.514700Z","iopub.execute_input":"2024-08-06T16:32:44.515122Z","iopub.status.idle":"2024-08-06T16:32:44.524800Z","shell.execute_reply.started":"2024-08-06T16:32:44.515092Z","shell.execute_reply":"2024-08-06T16:32:44.524012Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Processing","metadata":{}},{"cell_type":"code","source":"train_dataset = DataClass(\n    filename=TRAIN_FILEPATH,\n    max_length=MAX_LENGTH,\n    tokenizer_name=TOKENIZER_NAME,\n    use_prefix=USE_PREFIX\n)\ntrain_data_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n\nprint(\"The number of training batches: \", len(train_data_loader))\n\ndev_dataset = DataClass(\n    filename=DEV_FILEPATH,\n    max_length=MAX_LENGTH,\n    tokenizer_name=TOKENIZER_NAME,\n    use_prefix=USE_PREFIX\n)\ndev_data_loader = DataLoader(\n    dev_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False\n)\nprint(\"The number of validation batches: \", len(dev_data_loader))\nlabel_size = len(train_dataset.label_names)\n\ntest_dataset = DataClass(\n    filename=TEST_FILEPATH,\n    max_length=MAX_LENGTH,\n    tokenizer_name=TOKENIZER_NAME,\n    use_prefix=USE_PREFIX\n)\ntest_data_loader = DataLoader(\n    test_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False\n)\nprint(\"The number of Test batches: \", len(test_data_loader))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:32:44.525793Z","iopub.execute_input":"2024-08-06T16:32:44.526047Z","iopub.status.idle":"2024-08-06T16:32:55.470263Z","shell.execute_reply.started":"2024-08-06T16:32:44.526025Z","shell.execute_reply":"2024-08-06T16:32:55.469318Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20142ef8301744268fcae45d00e81f04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72f86c294725461ab3185c4cd2eeec78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"973c70fb59b24e3d929f75e5d417ccc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ca97ef6aca468baa9156e72f40ef1a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n","output_type":"stream"},{"name":"stdout","text":"Word statistics files not found!\nDownloading... done!\nUnpacking... ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n","output_type":"stream"},{"name":"stdout","text":"done!\nReading corpus_id_web_wiki - 1grams ...\ngenerating cache file for faster loading...\nreading ngrams /root/.ekphrasis/stats/corpus_id_web_wiki/counts_1grams.txt\nYou can't omit/backoff and unpack hashtags!\n unpack_hashtags will be set to False\n","output_type":"stream"},{"name":"stderr","text":"PreProcessing dataset ...: 100%|██████████| 1915/1915 [00:02<00:00, 790.21it/s]\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"The number of training batches:  120\nReading corpus_id_web_wiki - 1grams ...\nYou can't omit/backoff and unpack hashtags!\n unpack_hashtags will be set to False\n","output_type":"stream"},{"name":"stderr","text":"PreProcessing dataset ...: 100%|██████████| 525/525 [00:00<00:00, 782.21it/s]\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"The number of validation batches:  33\nReading corpus_id_web_wiki - 1grams ...\nYou can't omit/backoff and unpack hashtags!\n unpack_hashtags will be set to False\n","output_type":"stream"},{"name":"stderr","text":"PreProcessing dataset ...: 100%|██████████| 1079/1079 [00:01<00:00, 752.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"The number of Test batches:  68\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"#############################################################################\n# Start Training\n#############################################################################\n\n\n\ntest_results = {}\neval_results = {}\n\nfor lca, zlpr in zip(LCA_LOSSES, ZLPR_LOSSES):\n    \n    set_seed(seed)\n    \n    spanemo = SpanEmo(\n        output_dropout=DROPOUT_RATE,\n        lang=\"Indonesia\",\n        alpha=lca,\n        beta=zlpr,\n        embedding_vocab_size=len(train_dataset.tokenizer),\n        device=device,\n        encoder_name=ENCODER_NAME,\n    )\n\n    emorec = EmoRec(\n        output_dropout=DROPOUT_RATE,\n        lang=\"Indonesia\",\n        alpha=lca,\n        beta=zlpr,\n        embedding_vocab_size=len(train_dataset.tokenizer),\n        label_size=len(train_dataset.label_names),\n        device=device,\n        encoder_name=ENCODER_NAME,\n    )\n    \n    models = [spanemo, emorec]\n    \n    model = [x for x in models if x.name == MODEL_TO_TRAIN][0]\n\n    run_name = f\"{model.name}-roberta-BCE{str(1-lca-zlpr).replace('.','x')}-ZLPR{str(zlpr).replace('.','x')}-LCA{str(lca).replace('.','x')}\"\n    \n    early_stopping = EarlyStopping(filename = filename, patience=PATIENCE, delta=0, criteria=\"f1_Macro\", bigger_better=True)\n    \n    learn = Trainer(\n        model,\n        train_data_loader,\n        dev_data_loader,\n        filename=filename,\n        col_names=train_dataset.label_names,\n        early_stopping = early_stopping\n    )\n    \n    learn.fit(\n        classifier_learning_rate = CLASSIFIER_LEARNING_RATE,\n        encoder_learning_rate = ENCODER_LEARNING_RATE,\n        train_batch_size = TRAIN_BATCH_SIZE,\n        max_epoch = MAX_EPOCH,\n        project_name = PROJECT_NAME,\n        run_name = run_name,\n        device=device\n    )\n    \n    evaluate = EvaluateOnTest(\n        model, \n        test_data_loader, \n        model_path=MODEL_PATH, \n        col_names=train_dataset.label_names,\n        run_name = \"_test_\"+run_name\n    )\n    evaluate_dev = EvaluateOnTest(\n        model, \n        dev_data_loader, \n        model_path=MODEL_PATH, \n        col_names=train_dataset.label_names,\n        run_name = \"_dev_\"+run_name\n    )\n    print(\"==========\",run_name, \"===========\")\n    \n    test_results[run_name] = evaluate.predict(device=device)\n    eval_results[run_name] = evaluate_dev.predict(device=device)\n    \n    wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T16:32:55.473361Z","iopub.execute_input":"2024-08-06T16:32:55.473660Z","iopub.status.idle":"2024-08-06T18:22:48.349276Z","shell.execute_reply.started":"2024-08-06T16:32:55.473635Z","shell.execute_reply":"2024-08-06T18:22:48.348418Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0cf0c0b0ca04f638e2efde3ebd6721f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msyafiqfaray3\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111357941111161, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6008c1ce27c7427b9a60ca0f67527db8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/TA-Multi-Label-Emotion-Classification-Indonesian-Text/src/wandb/run-20240806_163307-n1lfcszv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/syafiqfaray3/TA%20Revisi/runs/n1lfcszv' target=\"_blank\">spanemo-roberta-BCE1x0-ZLPR0x0-LCA0x0</a></strong> to <a href='https://wandb.ai/syafiqfaray3/TA%20Revisi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/syafiqfaray3/TA%20Revisi' target=\"_blank\">https://wandb.ai/syafiqfaray3/TA%20Revisi</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/syafiqfaray3/TA%20Revisi/runs/n1lfcszv' target=\"_blank\">https://wandb.ai/syafiqfaray3/TA%20Revisi/runs/n1lfcszv</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Train_Loss</th>\n      <th>Val_Loss</th>\n      <th>F1-Macro</th>\n      <th>F1-Micro</th>\n      <th>Precision Macro</th>\n      <th>Recall Macro</th>\n      <th>JS-Macro</th>\n      <th>JS-Samples</th>\n      <th>MCC</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.3951</td>\n      <td>0.3215</td>\n      <td>0.2645</td>\n      <td>0.5614</td>\n      <td>0.8688</td>\n      <td>0.2567</td>\n      <td>0.1991</td>\n      <td>0.4073</td>\n      <td>0.5029</td>\n      <td>02:34</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.3025</td>\n      <td>0.3167</td>\n      <td>0.4271</td>\n      <td>0.6744</td>\n      <td>0.7732</td>\n      <td>0.4461</td>\n      <td>0.3218</td>\n      <td>0.5514</td>\n      <td>0.5998</td>\n      <td>02:39</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.2397</td>\n      <td>0.2890</td>\n      <td>0.4956</td>\n      <td>0.7016</td>\n      <td>0.7116</td>\n      <td>0.4716</td>\n      <td>0.3790</td>\n      <td>0.6111</td>\n      <td>0.6408</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.1740</td>\n      <td>0.3467</td>\n      <td>0.5690</td>\n      <td>0.7070</td>\n      <td>0.6250</td>\n      <td>0.6450</td>\n      <td>0.4274</td>\n      <td>0.6043</td>\n      <td>0.6388</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.1158</td>\n      <td>0.3184</td>\n      <td>0.6150</td>\n      <td>0.7133</td>\n      <td>0.6522</td>\n      <td>0.5971</td>\n      <td>0.4651</td>\n      <td>0.6243</td>\n      <td>0.6493</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.0807</td>\n      <td>0.4377</td>\n      <td>0.6185</td>\n      <td>0.7064</td>\n      <td>0.6413</td>\n      <td>0.6555</td>\n      <td>0.4618</td>\n      <td>0.6081</td>\n      <td>0.6368</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.0580</td>\n      <td>0.4993</td>\n      <td>0.5837</td>\n      <td>0.7109</td>\n      <td>0.6869</td>\n      <td>0.5544</td>\n      <td>0.4331</td>\n      <td>0.6097</td>\n      <td>0.6455</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.0361</td>\n      <td>0.6249</td>\n      <td>0.6312</td>\n      <td>0.7190</td>\n      <td>0.6484</td>\n      <td>0.6320</td>\n      <td>0.4767</td>\n      <td>0.6351</td>\n      <td>0.6551</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.0324</td>\n      <td>0.6180</td>\n      <td>0.6336</td>\n      <td>0.7265</td>\n      <td>0.6526</td>\n      <td>0.6279</td>\n      <td>0.4776</td>\n      <td>0.6490</td>\n      <td>0.6650</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.0139</td>\n      <td>0.7812</td>\n      <td>0.6332</td>\n      <td>0.7274</td>\n      <td>0.6539</td>\n      <td>0.6229</td>\n      <td>0.4790</td>\n      <td>0.6390</td>\n      <td>0.6655</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.0141</td>\n      <td>0.8565</td>\n      <td>0.6082</td>\n      <td>0.7189</td>\n      <td>0.7046</td>\n      <td>0.5509</td>\n      <td>0.4567</td>\n      <td>0.6192</td>\n      <td>0.6617</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.0084</td>\n      <td>0.9082</td>\n      <td>0.6077</td>\n      <td>0.7297</td>\n      <td>0.7230</td>\n      <td>0.5451</td>\n      <td>0.4576</td>\n      <td>0.6378</td>\n      <td>0.6745</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.0078</td>\n      <td>0.9346</td>\n      <td>0.6249</td>\n      <td>0.7289</td>\n      <td>0.6767</td>\n      <td>0.6070</td>\n      <td>0.4721</td>\n      <td>0.6268</td>\n      <td>0.6668</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.0042</td>\n      <td>1.0087</td>\n      <td>0.6106</td>\n      <td>0.7217</td>\n      <td>0.6747</td>\n      <td>0.5747</td>\n      <td>0.4592</td>\n      <td>0.6319</td>\n      <td>0.6621</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.0033</td>\n      <td>1.0434</td>\n      <td>0.6166</td>\n      <td>0.7193</td>\n      <td>0.6589</td>\n      <td>0.5932</td>\n      <td>0.4637</td>\n      <td>0.6159</td>\n      <td>0.6566</td>\n      <td>02:39</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.0078</td>\n      <td>0.9860</td>\n      <td>0.6261</td>\n      <td>0.7376</td>\n      <td>0.6717</td>\n      <td>0.6002</td>\n      <td>0.4752</td>\n      <td>0.6437</td>\n      <td>0.6784</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.0007</td>\n      <td>1.0324</td>\n      <td>0.6283</td>\n      <td>0.7362</td>\n      <td>0.7063</td>\n      <td>0.5824</td>\n      <td>0.4755</td>\n      <td>0.6394</td>\n      <td>0.6786</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.0010</td>\n      <td>1.0522</td>\n      <td>0.6264</td>\n      <td>0.7351</td>\n      <td>0.6951</td>\n      <td>0.5906</td>\n      <td>0.4734</td>\n      <td>0.6371</td>\n      <td>0.6759</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.0002</td>\n      <td>1.0526</td>\n      <td>0.6230</td>\n      <td>0.7327</td>\n      <td>0.6912</td>\n      <td>0.5842</td>\n      <td>0.4706</td>\n      <td>0.6352</td>\n      <td>0.6737</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.0001</td>\n      <td>1.0560</td>\n      <td>0.6237</td>\n      <td>0.7335</td>\n      <td>0.6910</td>\n      <td>0.5866</td>\n      <td>0.4710</td>\n      <td>0.6349</td>\n      <td>0.6744</td>\n      <td>02:40</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}},{"name":"stdout","text":"epoch#:  1\nf1_Macro increased (inf --> 0.264485).  Saving model ...\nepoch#:  2\nf1_Macro increased (0.264485 --> 0.427058).  Saving model ...\nepoch#:  3\nf1_Macro increased (0.427058 --> 0.495564).  Saving model ...\nepoch#:  4\nf1_Macro increased (0.495564 --> 0.568970).  Saving model ...\nepoch#:  5\nf1_Macro increased (0.568970 --> 0.614977).  Saving model ...\nepoch#:  6\nf1_Macro increased (0.614977 --> 0.618538).  Saving model ...\nepoch#:  7\nEarlyStopping counter: 1 out of 20\nepoch#:  8\nf1_Macro increased (0.618538 --> 0.631235).  Saving model ...\nepoch#:  9\nf1_Macro increased (0.631235 --> 0.633558).  Saving model ...\nepoch#:  10\nEarlyStopping counter: 1 out of 20\nepoch#:  11\nEarlyStopping counter: 2 out of 20\nepoch#:  12\nEarlyStopping counter: 3 out of 20\nepoch#:  13\nEarlyStopping counter: 4 out of 20\nepoch#:  14\nEarlyStopping counter: 5 out of 20\nepoch#:  15\nEarlyStopping counter: 6 out of 20\nepoch#:  16\nEarlyStopping counter: 7 out of 20\nepoch#:  17\nEarlyStopping counter: 8 out of 20\nepoch#:  18\nEarlyStopping counter: 9 out of 20\nepoch#:  19\nEarlyStopping counter: 10 out of 20\nepoch#:  20\nEarlyStopping counter: 11 out of 20\n========== spanemo-roberta-BCE1x0-ZLPR0x0-LCA0x0 ===========\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"F1-Macro: 0.6369 F1-Micro: 0.7246 Precision Macro: 0.6630 Recall Macro: 0.6252 JS-Macro: 0.4798 JS-Samples: 0.6425 MCC: 0.6637 Time: 00:28\nConfusion Matrix:\n[[[ 558   65]\n  [  81  375]]\n\n [[ 978   17]\n  [  51   33]]\n\n [[1016   23]\n  [  17   23]]\n\n [[ 635   83]\n  [ 109  252]]\n\n [[ 835   72]\n  [  32  140]]\n\n [[1018   17]\n  [  24   20]]\n\n [[ 765   72]\n  [  92  150]]]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"F1-Macro: 0.6336 F1-Micro: 0.7265 Precision Macro: 0.6526 Recall Macro: 0.6279 JS-Macro: 0.4776 JS-Samples: 0.6490 MCC: 0.6650 Time: 00:13\nConfusion Matrix:\n[[[262  35]\n  [ 40 188]]\n\n [[476   9]\n  [ 23  17]]\n\n [[491  13]\n  [ 10  11]]\n\n [[311  38]\n  [ 47 129]]\n\n [[413  31]\n  [ 16  65]]\n\n [[491   8]\n  [ 15  11]]\n\n [[372  49]\n  [ 35  69]]]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>f1_Macro</td><td>▁▄▅▇██▇█████████████</td></tr><tr><td>f1_Micro</td><td>▁▅▇▇▇▇▇▇██▇██▇▇█████</td></tr><tr><td>js_Macro</td><td>▁▄▅▇██▇███▇▇████████</td></tr><tr><td>js_Samples</td><td>▁▅▇▇▇▇▇███▇█▇█▇█████</td></tr><tr><td>mcc</td><td>▁▅▆▆▇▆▇▇▇▇▇██▇▇█████</td></tr><tr><td>precision_macro</td><td>█▅▃▁▂▁▃▂▂▂▃▄▂▂▂▂▃▃▃▃</td></tr><tr><td>recall_macro</td><td>▁▄▅█▇█▆██▇▆▆▇▇▇▇▇▇▇▇</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▂▁▂▃▄▄▅▆▇▇██▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>f1_Macro</td><td>0.62372</td></tr><tr><td>f1_Micro</td><td>0.73353</td></tr><tr><td>js_Macro</td><td>0.47099</td></tr><tr><td>js_Samples</td><td>0.63492</td></tr><tr><td>mcc</td><td>0.67442</td></tr><tr><td>precision_macro</td><td>0.69105</td></tr><tr><td>recall_macro</td><td>0.58656</td></tr><tr><td>train_loss</td><td>0.00011</td></tr><tr><td>val_loss</td><td>1.05601</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">spanemo-roberta-BCE1x0-ZLPR0x0-LCA0x0</strong> at: <a href='https://wandb.ai/syafiqfaray3/TA%20Revisi/runs/n1lfcszv' target=\"_blank\">https://wandb.ai/syafiqfaray3/TA%20Revisi/runs/n1lfcszv</a><br/> View project at: <a href='https://wandb.ai/syafiqfaray3/TA%20Revisi' target=\"_blank\">https://wandb.ai/syafiqfaray3/TA%20Revisi</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240806_163307-n1lfcszv/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/TA-Multi-Label-Emotion-Classification-Indonesian-Text/src/wandb/run-20240806_172757-drufx1z1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/syafiqfaray3/TA%20Revisi/runs/drufx1z1' target=\"_blank\">spanemo-roberta-BCE0x0-ZLPR0x4-LCA0x6</a></strong> to <a href='https://wandb.ai/syafiqfaray3/TA%20Revisi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/syafiqfaray3/TA%20Revisi' target=\"_blank\">https://wandb.ai/syafiqfaray3/TA%20Revisi</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/syafiqfaray3/TA%20Revisi/runs/drufx1z1' target=\"_blank\">https://wandb.ai/syafiqfaray3/TA%20Revisi/runs/drufx1z1</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Train_Loss</th>\n      <th>Val_Loss</th>\n      <th>F1-Macro</th>\n      <th>F1-Micro</th>\n      <th>Precision Macro</th>\n      <th>Recall Macro</th>\n      <th>JS-Macro</th>\n      <th>JS-Samples</th>\n      <th>MCC</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.3626</td>\n      <td>1.1938</td>\n      <td>0.3975</td>\n      <td>0.6090</td>\n      <td>0.7094</td>\n      <td>0.3906</td>\n      <td>0.2874</td>\n      <td>0.4835</td>\n      <td>0.5276</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.1425</td>\n      <td>1.1860</td>\n      <td>0.3940</td>\n      <td>0.6603</td>\n      <td>0.8219</td>\n      <td>0.4142</td>\n      <td>0.2998</td>\n      <td>0.5238</td>\n      <td>0.5834</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.9515</td>\n      <td>1.1502</td>\n      <td>0.5017</td>\n      <td>0.7060</td>\n      <td>0.5792</td>\n      <td>0.4748</td>\n      <td>0.3819</td>\n      <td>0.6184</td>\n      <td>0.6431</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.7403</td>\n      <td>1.3489</td>\n      <td>0.5903</td>\n      <td>0.7028</td>\n      <td>0.6248</td>\n      <td>0.6303</td>\n      <td>0.4411</td>\n      <td>0.6046</td>\n      <td>0.6325</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.5891</td>\n      <td>1.1943</td>\n      <td>0.6206</td>\n      <td>0.7425</td>\n      <td>0.6648</td>\n      <td>0.6097</td>\n      <td>0.4754</td>\n      <td>0.6606</td>\n      <td>0.6836</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.4604</td>\n      <td>1.5433</td>\n      <td>0.6306</td>\n      <td>0.7232</td>\n      <td>0.6520</td>\n      <td>0.6370</td>\n      <td>0.4758</td>\n      <td>0.6340</td>\n      <td>0.6583</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.3893</td>\n      <td>1.8036</td>\n      <td>0.5838</td>\n      <td>0.7187</td>\n      <td>0.7267</td>\n      <td>0.5390</td>\n      <td>0.4334</td>\n      <td>0.6108</td>\n      <td>0.6565</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.3304</td>\n      <td>2.2791</td>\n      <td>0.6160</td>\n      <td>0.7226</td>\n      <td>0.6766</td>\n      <td>0.5899</td>\n      <td>0.4632</td>\n      <td>0.6356</td>\n      <td>0.6602</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.3171</td>\n      <td>2.5370</td>\n      <td>0.6571</td>\n      <td>0.7297</td>\n      <td>0.6391</td>\n      <td>0.6850</td>\n      <td>0.5004</td>\n      <td>0.6537</td>\n      <td>0.6664</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.2823</td>\n      <td>2.8263</td>\n      <td>0.6303</td>\n      <td>0.7277</td>\n      <td>0.6870</td>\n      <td>0.6025</td>\n      <td>0.4772</td>\n      <td>0.6473</td>\n      <td>0.6675</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.2534</td>\n      <td>3.2351</td>\n      <td>0.6250</td>\n      <td>0.7263</td>\n      <td>0.6801</td>\n      <td>0.5985</td>\n      <td>0.4716</td>\n      <td>0.6419</td>\n      <td>0.6653</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.2430</td>\n      <td>3.4524</td>\n      <td>0.6332</td>\n      <td>0.7332</td>\n      <td>0.7050</td>\n      <td>0.6002</td>\n      <td>0.4790</td>\n      <td>0.6438</td>\n      <td>0.6730</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.2392</td>\n      <td>3.5604</td>\n      <td>0.6468</td>\n      <td>0.7382</td>\n      <td>0.7093</td>\n      <td>0.6215</td>\n      <td>0.4928</td>\n      <td>0.6541</td>\n      <td>0.6788</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.2278</td>\n      <td>3.6883</td>\n      <td>0.6409</td>\n      <td>0.7348</td>\n      <td>0.6734</td>\n      <td>0.6269</td>\n      <td>0.4894</td>\n      <td>0.6498</td>\n      <td>0.6742</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.2262</td>\n      <td>3.7451</td>\n      <td>0.6588</td>\n      <td>0.7465</td>\n      <td>0.7276</td>\n      <td>0.6214</td>\n      <td>0.5074</td>\n      <td>0.6644</td>\n      <td>0.6897</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.2257</td>\n      <td>3.9660</td>\n      <td>0.6408</td>\n      <td>0.7363</td>\n      <td>0.6954</td>\n      <td>0.6146</td>\n      <td>0.4903</td>\n      <td>0.6517</td>\n      <td>0.6770</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.2186</td>\n      <td>3.9892</td>\n      <td>0.6485</td>\n      <td>0.7396</td>\n      <td>0.7208</td>\n      <td>0.6148</td>\n      <td>0.4963</td>\n      <td>0.6584</td>\n      <td>0.6813</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.2195</td>\n      <td>4.0766</td>\n      <td>0.6586</td>\n      <td>0.7439</td>\n      <td>0.7175</td>\n      <td>0.6269</td>\n      <td>0.5057</td>\n      <td>0.6590</td>\n      <td>0.6864</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.2156</td>\n      <td>4.0438</td>\n      <td>0.6526</td>\n      <td>0.7380</td>\n      <td>0.6993</td>\n      <td>0.6277</td>\n      <td>0.4991</td>\n      <td>0.6514</td>\n      <td>0.6788</td>\n      <td>02:40</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.2172</td>\n      <td>4.0401</td>\n      <td>0.6482</td>\n      <td>0.7387</td>\n      <td>0.7002</td>\n      <td>0.6208</td>\n      <td>0.4968</td>\n      <td>0.6505</td>\n      <td>0.6799</td>\n      <td>02:40</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}},{"name":"stdout","text":"epoch#:  1\nf1_Macro increased (inf --> 0.397491).  Saving model ...\nepoch#:  2\nEarlyStopping counter: 1 out of 20\nepoch#:  3\nf1_Macro increased (0.397491 --> 0.501696).  Saving model ...\nepoch#:  4\nf1_Macro increased (0.501696 --> 0.590312).  Saving model ...\nepoch#:  5\nf1_Macro increased (0.590312 --> 0.620592).  Saving model ...\nepoch#:  6\nf1_Macro increased (0.620592 --> 0.630581).  Saving model ...\nepoch#:  7\nEarlyStopping counter: 1 out of 20\nepoch#:  8\nEarlyStopping counter: 2 out of 20\nepoch#:  9\nf1_Macro increased (0.630581 --> 0.657106).  Saving model ...\nepoch#:  10\nEarlyStopping counter: 1 out of 20\nepoch#:  11\nEarlyStopping counter: 2 out of 20\nepoch#:  12\nEarlyStopping counter: 3 out of 20\nepoch#:  13\nEarlyStopping counter: 4 out of 20\nepoch#:  14\nEarlyStopping counter: 5 out of 20\nepoch#:  15\nf1_Macro increased (0.657106 --> 0.658791).  Saving model ...\nepoch#:  16\nEarlyStopping counter: 1 out of 20\nepoch#:  17\nEarlyStopping counter: 2 out of 20\nepoch#:  18\nEarlyStopping counter: 3 out of 20\nepoch#:  19\nEarlyStopping counter: 4 out of 20\nepoch#:  20\nEarlyStopping counter: 5 out of 20\n========== spanemo-roberta-BCE0x0-ZLPR0x4-LCA0x6 ===========\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"F1-Macro: 0.6532 F1-Micro: 0.7410 Precision Macro: 0.7151 Recall Macro: 0.6167 JS-Macro: 0.4964 JS-Samples: 0.6574 MCC: 0.6828 Time: 00:28\nConfusion Matrix:\n[[[ 541   82]\n  [  69  387]]\n\n [[ 979   16]\n  [  43   41]]\n\n [[1033    6]\n  [  24   16]]\n\n [[ 608  110]\n  [  73  288]]\n\n [[ 857   50]\n  [  47  125]]\n\n [[1023   12]\n  [  25   19]]\n\n [[ 767   70]\n  [  91  151]]]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"F1-Macro: 0.6588 F1-Micro: 0.7465 Precision Macro: 0.7276 Recall Macro: 0.6214 JS-Macro: 0.5074 JS-Samples: 0.6644 MCC: 0.6897 Time: 00:13\nConfusion Matrix:\n[[[256  41]\n  [ 29 199]]\n\n [[477   8]\n  [ 24  16]]\n\n [[501   3]\n  [ 13   8]]\n\n [[299  50]\n  [ 39 137]]\n\n [[432  12]\n  [ 16  65]]\n\n [[494   5]\n  [ 13  13]]\n\n [[373  48]\n  [ 40  64]]]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.020 MB of 0.020 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>f1_Macro</td><td>▁▁▄▆▇▇▆▇█▇▇▇████████</td></tr><tr><td>f1_Micro</td><td>▁▄▆▆█▇▇▇▇▇▇▇█▇█▇████</td></tr><tr><td>js_Macro</td><td>▁▁▄▆▇▇▆▇█▇▇▇█▇█▇████</td></tr><tr><td>js_Samples</td><td>▁▃▆▆█▇▆▇█▇▇▇█▇████▇▇</td></tr><tr><td>mcc</td><td>▁▃▆▆█▇▇▇▇▇▇▇█▇█▇████</td></tr><tr><td>precision_macro</td><td>▅█▁▂▃▃▅▄▃▄▄▅▅▄▅▄▅▅▄▄</td></tr><tr><td>recall_macro</td><td>▁▂▃▇▆▇▅▆█▆▆▆▆▇▆▆▆▇▇▆</td></tr><tr><td>train_loss</td><td>█▇▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▂▃▄▄▅▆▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>f1_Macro</td><td>0.64817</td></tr><tr><td>f1_Micro</td><td>0.73871</td></tr><tr><td>js_Macro</td><td>0.49684</td></tr><tr><td>js_Samples</td><td>0.65048</td></tr><tr><td>mcc</td><td>0.67987</td></tr><tr><td>precision_macro</td><td>0.70016</td></tr><tr><td>recall_macro</td><td>0.62083</td></tr><tr><td>train_loss</td><td>0.21719</td></tr><tr><td>val_loss</td><td>4.04006</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">spanemo-roberta-BCE0x0-ZLPR0x4-LCA0x6</strong> at: <a href='https://wandb.ai/syafiqfaray3/TA%20Revisi/runs/drufx1z1' target=\"_blank\">https://wandb.ai/syafiqfaray3/TA%20Revisi/runs/drufx1z1</a><br/> View project at: <a href='https://wandb.ai/syafiqfaray3/TA%20Revisi' target=\"_blank\">https://wandb.ai/syafiqfaray3/TA%20Revisi</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240806_172757-drufx1z1/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"import pickle \nwith open(f'{model.name}_test.pkl', 'wb') as f:\n    pickle.dump(test_results, f)\n    \nwith open(f'{model.name}_eval.pkl', 'wb') as f:\n    pickle.dump(eval_results, f)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T18:22:48.350699Z","iopub.execute_input":"2024-08-06T18:22:48.351025Z","iopub.status.idle":"2024-08-06T18:22:48.357661Z","shell.execute_reply.started":"2024-08-06T18:22:48.350996Z","shell.execute_reply":"2024-08-06T18:22:48.356839Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"evaluate = EvaluateOnTest(\n        model, \n        test_data_loader, \n        model_path=MODEL_PATH, \n        col_names=train_dataset.label_names\n    )\nevaluate.predict(device=device)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T18:22:48.358741Z","iopub.execute_input":"2024-08-06T18:22:48.359125Z","iopub.status.idle":"2024-08-06T18:23:17.193971Z","shell.execute_reply.started":"2024-08-06T18:22:48.359061Z","shell.execute_reply":"2024-08-06T18:23:17.192988Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"F1-Macro: 0.6532 F1-Micro: 0.7410 Precision Macro: 0.7151 Recall Macro: 0.6167 JS-Macro: 0.4964 JS-Samples: 0.6574 MCC: 0.6828 Time: 00:28\nConfusion Matrix:\n[[[ 541   82]\n  [  69  387]]\n\n [[ 979   16]\n  [  43   41]]\n\n [[1033    6]\n  [  24   16]]\n\n [[ 608  110]\n  [  73  288]]\n\n [[ 857   50]\n  [  47  125]]\n\n [[1023   12]\n  [  25   19]]\n\n [[ 767   70]\n  [  91  151]]]\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'f1-macro': 0.6532478476681745,\n 'f1-micro': 0.7409812409812411,\n 'precision-macro': 0.7151136909564828,\n 'recall-macro': 0.6167275274504304,\n 'js-macro': 0.4964207209215874,\n 'js-samples': 0.657398826073525,\n 'mcc': 0.6828194856643677}"},"metadata":{}}]}]}